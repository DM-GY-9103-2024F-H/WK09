{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QshK8s21WBrf"
      },
      "source": [
        "# Week 09\n",
        "\n",
        "Unsupervised Learning: Distances, Clustering and PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hf8SXUwWOho"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Run the following 2 cells to import all necessary libraries and helpers for this week's exercises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget -q https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/src/data_utils.py\n",
        "!wget -q https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/src/image_utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from data_utils import PCA, RandomForestClassifier, StandardScaler\n",
        "from data_utils import KMeansClustering, GaussianClustering, SpectralClustering\n",
        "from data_utils import object_from_json_url, classification_error, display_confusion_matrix\n",
        "\n",
        "from image_utils import make_image, open_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cost and Distance Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The concept of **_distance_** is something that we saw and used in previous exercises but didn't talk too much about.\n",
        "\n",
        "**_Distance_** is how we tell how close two data points are to each other, and is the basis for clustering, classification and regression algorithms.\n",
        "\n",
        "In classification, we learn how to label new data based on how \"close\" it is to our already-labeled training data. In regression, we find parameters to equations that make our line-of-best-fit \"close\" to all of the points in the dataset. In clustering, we group data points in a way that minimizes distances between points within a cluster, while maximizing the distances between clusters. Distance is also an important concept for recommendation systems where we want to calculate when someone's taste is close to someone else's.\n",
        "\n",
        "### 1D\n",
        "\n",
        "The concept of distance in one dimension is pretty easy to understand: it's how far two points on a line are to each other. Physically, we can think of $1D$ distance as the distance between runners in a race, or, we can even think of time as a one-dimensional space where we measure distance between events in seconds, or minutes, or days.\n",
        "\n",
        "<img src=\"./imgs/dist1d.jpg\" height=\"220px\" />\n",
        "\n",
        "Each point in 1-dimensional space is described with a single variable, and the distance between any two points is just the absolute value of their difference:\n",
        "\n",
        "$\\displaystyle D(x_0, x_1) = |x_0 - x_1|$<br>\n",
        "$\\displaystyle D(x_0, x_2) = |x_0 - x_2|$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2D\n",
        "\n",
        "Two-dimensional distances, where we have $2$ variables to describe each of our points, is also pretty familiar to us. This can be the distance between two cities on a map, measured in angles of longitude and latitude, or distances between two points in Manhattan, measured in streets and avenues.\n",
        "\n",
        "<img src=\"./imgs/dist2d.jpg\" width=\"95%\" />\n",
        "\n",
        "We have $2$ variables for each of our points and we also have $2$ ways in which we can combine them to measure distances in $2D$. The first is called $L1$, or Manhattan, distance, and it's the sum of the distances in each of the separate dimensions.\n",
        "\n",
        "$\\displaystyle D_{L1}(x_0y_0, x_1y_1) = |x_0 - x_1| + |y_0 - y_1|$\n",
        "\n",
        "The other way of measuring distances in $2D$ is using the $L2$, or Euclidean, distance formula:\n",
        "\n",
        "$\\displaystyle D_{L2}(x_0y_0, x_2y_2) = \\sqrt{(x_0 - x_2)^2 + (y_0 - y_2)^2}$\n",
        "\n",
        "This is pretty easy to understand as distances on a map, but... what if $x$ is a variable for $height$ in our dataset, and $y$ is the variable for $ear\\ length$? The calculations are still valid. As long as we remember to normalize our data, we can use the $L1$ or $L2$ formulas to figure out how \"close\" our data points are."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3D\n",
        "\n",
        "Three-dimensional points have $3$ variables that describe them, and, while less common, it's still easy to understand how to measure the distance between them. We could be talking about the distance between planets, or between atoms, or between a wifi router and a cellphone. As long as the points aren't on a plane, we need $3$ variables to describe them and measure the distance between them.\n",
        "\n",
        "<img src=\"./imgs/dist3d.jpg\" width=\"95%\" />\n",
        "\n",
        "We can extend the $L1$ and $L2$ distance formulas to work in $3D$:\n",
        "\n",
        "$\\displaystyle D_{L1}(x_0y_0z_0, x_1y_1z_1) = |x_0 - x_1| + |y_0 - y_1| + |z_0 - z_1|$\n",
        "\n",
        "$\\displaystyle D_{L2}(x_0y_0z_0, x_2y_2z_2) = \\sqrt{(x_0 - x_2)^2 + (y_0 - y_2)^2 + (z_0 - z_2)^2}$\n",
        "\n",
        "This works even when our $3$ variables aren't actually physical locations. If $x$ is a variable that keeps track of the number of rooms in a house, $y$ a variable for the age of the house, and $z$ the total area of the house, we can use the above formulas to measure how \"close\" two houses in our dataset are (after we normalize our data, of course).\n",
        "\n",
        "### N-Dimensions\n",
        "\n",
        "Most of the datasets we've seen so far already have more then $3$ features/dimensions... what then?\n",
        "\n",
        "Well... the $L1$ and $L2$ distance formulas can be used regardless of the number of features/dimensions in our dataset. We just keep adding parameters to our formula:\n",
        "\n",
        "$\\displaystyle D_{L1} = \\sum_{d}{|A_d - B_d|}$ (for all dimensions $d$)\n",
        "\n",
        "$\\displaystyle D_{L2} = \\sqrt{\\sum_{d}{(A_d - B_d)^2}}$ (for all dimensions $d$)\n",
        "\n",
        "So even when we have a dataset with $15$ or $20$ features/dimensions, we can still get some idea of how \"close\" two points in that dataset are."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Other Distance Formulas\n",
        "\n",
        "$L1$ and $L2$ are definitely the most widely used distance formulas in ML applications, but they aren't the only ones. Two other types of distances are:\n",
        "\n",
        "#### Cosine Similarity\n",
        "\n",
        "When we're dealing with datasets that are very sparse (there are more dimensions than points), and the $L1$ and $L2$ distances that separate the data points are really huge, we might want to measure the cosine similarity between two points instead.\n",
        "\n",
        "<img src=\"./imgs/distcos.jpg\" height=\"300px\" />\n",
        "\n",
        "In the drawing above, instead of measuring the direct distances between $x_0y_0$, $x_1y_1$ and $x_2y_2$, we can pick a separate reference point and measure the cosine of the angles formed by lines drawn from the reference point to each of the other points. We can see that angle $\\theta_{12}$ is smaller than $\\theta_{01}$ and $\\theta_{02}$, which means that $(x_1y_1, x_2y_2)$ is the pair of most similar points.\n",
        "\n",
        "Points with cosine values close to $1$ are in the same direction in space; points with cosine values close to $0$ are in perpendicular directions, and points with cosine values close to $-1$ are in opposite directions.\n",
        "\n",
        "$\\displaystyle cos(A, B) = \\frac{A \\cdot B}{ \\left|\\left|A\\right|\\right| \\left|\\left|B\\right|\\right|}$\n",
        "\n",
        "$\\displaystyle cos(x_0y_0, x_1y_1) = \\frac{x_0x_1 + y_0y_1}{\\sqrt{x_0^2+y_0^2} \\sqrt{x_1^2+y_1^2}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Mahalanobis Distance\n",
        "\n",
        "This is useful for measuring distances between points that are part of a collection of points.\n",
        "\n",
        "<img src=\"./imgs/distmana-01.jpg\" height=\"250px\" />\n",
        "\n",
        "In the drawing above, if we only had the points $x_0y_0$, $x_1y_1$ and $x_2y_2$, we could use $L2$ distances and everything is fine:\n",
        "\n",
        "$\\displaystyle D_{L2}(x_0y_0, x_1y_1) > D_{L2}(x_0y_0, x_2y_2)$.\n",
        "\n",
        "But, if instead, $x_0y_0$, $x_1y_1$ and $x_2y_2$ are part of a collection of points with a well-defined average and standard deviation, like the image below, we should use something that makes more common distances shorter, and rarer distances larger.\n",
        "\n",
        "<img src=\"./imgs/distmana-02.jpg\" height=\"250px\" />\n",
        "\n",
        "In this case, where $x_0y_0$ and $x_2y_2$ are on the extremes of the distribution, we want the distance between them to be larger than the distance between $x_0y_0$ and $x_1y_1$, which happens along a more common direction of the data.\n",
        "\n",
        "In order to have $\\displaystyle D_{M}(x_0y_0, x_2y_2) > D_{M}(x_0y_0, x_1y_1)$, we have to take into account the distribution of our data: its mean, standard deviation and covariances. One way to do that is using the formula below:\n",
        "\n",
        "$\\displaystyle D_{M}(A, B) = \\sqrt{(A - B)^2 V_I}$.\n",
        "\n",
        "Where $V_I$ is the inverse of the covariance matrix of our points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Let's practice\n",
        "\n",
        "We're going to download the `ANSUR` dataset again, select the first person on the dataset and find the $5$ people that are the most similar to this person by $L2$ distance and by cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "ANSUR_FILE = \"https://raw.githubusercontent.com/DM-GY-9103-2024F-H/9103-utils/main/datasets/json/ansur.json\"\n",
        "ansur_data = object_from_json_url(ANSUR_FILE)\n",
        "\n",
        "# Read into DataFrame\n",
        "ansur_df = pd.json_normalize(ansur_data)\n",
        "ansur_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare the data, measure distances, re-order DataFrame\n",
        "\n",
        "Follow these steps:\n",
        "\n",
        "1. Before we do anything else, let's drop the `gender` column and normalize the rest of the data.\n",
        "\n",
        "2. Then, we'll define two functions, `l2_dist()` and `cos_sim()`. They will each take $2$ data points and return the distance/similarity between them.\n",
        "   - HINT: Remember the `zip()` function to iterate over $2$ lists simultaneously. In both of the functions we want to sum up a bunch of terms while we iterate over the lists of features, before we do the final square roots or divisions.\n",
        "\n",
        "3. Use these functions to compute $L2$ distances and cosine similarities between the first point and all other points.\n",
        "   - HINT: We can get a list of all of the rows in our `DataFrame` by accessing its `values` variable. So, this will give us our first row of data: `ansur_df.values[0]`, and this is a way to iterate over all rows: `for r in ansur_df.values`.\n",
        "\n",
        "4. The result of running the functions on all rows of our data will most likely be in two lists. We can put those lists into new columns of our `DataFrame`. Create `p0-l2` and `p0-cos` columns in the `DataFrame` to store the values of $L2$ distances and cosine similarities.\n",
        "   - HINT: This is just `ansur_df[\"new-column-name\"] = list_of_values`.\n",
        "\n",
        "5. Re-order the `DataFrame` by these $2$ columns and take a look at the points closest to the first point.\n",
        "  - HINT: Look up [`sort_values()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html) \n",
        "  - HINT: cosine of $1.0$ means same direction, while cosine of $-1.0$ means in the opposite direction.\n",
        "\n",
        "Are the resulting sets of points the same ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TODO: 1. drop gender and normalize the rest of the data\n",
        "\n",
        "## TODO: 2. fill in the following functions:\n",
        "\n",
        "def l2_dist(A, B):\n",
        "  ## TODO: implement this and remove the print() statement\n",
        "  print(A, B)\n",
        "\n",
        "def cos_sim(A, B):\n",
        "  ## TODO: implement this and remove the print() statement\n",
        "  print(A, B)\n",
        "\n",
        "## TODO: 3. create lists of distances/similarities\n",
        "\n",
        "## TODO: 4. put lists of distances/similarities back into DataFrame\n",
        "  \n",
        "## TODO: 5. order DataFrame by L2 distances\n",
        "\n",
        "## TODO: 5. order DataFrame by cosine similarity\n",
        "\n",
        "## TODO: compare closest 5 points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Unsupervised Learning\n",
        "\n",
        "### Clustering\n",
        "\n",
        "#### More Wine ! üç∑üç∑üç∑\n",
        "\n",
        "Let's pretend we own an online wine store.\n",
        "\n",
        "Last week we created a model that predicts wine quality based on a bunch of its properties. We could use this model to figure out how much to pay suppliers for the wine, and how much to charge costumers.\n",
        "\n",
        "But, maybe this \"`quality`\" feature might not be something we want to share with our costumers. Even though it's based on data, it sounds abstract and subjective and would require explanations about our data and our process, which could create confusion.\n",
        "\n",
        "Using all six features from the original dataset (`alcohol`, `acidity`, `density`, etc) might also not be very useful for costumers who want to buy new wines that are similar to ones that they have previously liked.\n",
        "\n",
        "What we can do instead is classify the wines into groups that take into account all of the features of the dataset, but present costumers with a more manageable amount of information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Recommendations\n",
        "\n",
        "What we're really hoping to have is a simple recommendation system for our costumers, where we can recommend wines based on previous wines they liked, without them having to know the $6$ features of the previous wines.\n",
        "\n",
        "There are a few ways of doing this, but the strategy we'll take is called clustering.\n",
        "\n",
        "### Clustering\n",
        "\n",
        "[Clustering](https://en.wikipedia.org/wiki/Cluster_analysis), or cluster analysis, is an example of an *unsupervised* learning method that groups items based on their many features and properties.\n",
        "\n",
        "We'll use it to divide our wines in such a way that wines in the same group, or *cluster*, are more similar to each other than to wines in other clusters.\n",
        "\n",
        "These clusters won't necessarily correlate directly to the features in our dataset, but will be computed using a combination of the features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Supervised Learning\n",
        "\n",
        "The models that we've trained so far for doing regression and classification are considered *supervised* models. During training we give the model our input features, but also provide it with the *correct* values for the output signals. These output signals tend to be human-labeled values, and are sometimes called the *supervisory signals*.\n",
        "\n",
        "When fully-labeled training data is processed during training, we are hoping that the model learns to extrapolate what it *sees* in the labeled data to new, unseen, unlabeled instances of data with the same input features, but unknown output values.\n",
        "\n",
        "#### Supervised Classification:\n",
        "\n",
        "Given a set of initial data points with labels:<br>\n",
        "<img src=\"./imgs/classification-02.jpg\" width=\"620px\"/>\n",
        "\n",
        "We create a model that learns to assign labels to the original points:\n",
        "<img src=\"./imgs/classification-03.jpg\" width=\"620px\"/>\n",
        "\n",
        "so that later we can assign correct labels to new data points:\n",
        "<img src=\"./imgs/classification-04.jpg\" width=\"620px\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unsupervised Learning\n",
        "\n",
        "Unlike supervised learning, unsupervised models learn patterns from unlabeled data. This means all of the features are considered input features, and there are no separate output features or signals. The idea is that by analyzing and processing data in specific ways, the model is able to build a concise representation of its features and create new ways of interpreting, visualizing or generating similar data.\n",
        "\n",
        "We can use unsupervised learning models to explore new datasets and try to simplify our data before we do any kind of supervised learning.\n",
        "\n",
        "We can also use supervised learning to build recommendation systems that learn how to group items by their many features or characteristics.\n",
        "\n",
        "The steps for training an unsupervised model should seem familiar:\n",
        "\n",
        "1. Load dataset\n",
        "2. Encode label features as numbers\n",
        "3. Normalize the data\n",
        "4. Select variables and features to be considered\n",
        "5. Create a model\n",
        "6. Run model on input data and test data\n",
        "7. Measure error\n",
        "\n",
        "Even though it all looks familiar, that last step isn't very obvious.\n",
        "\n",
        "How do we measure error on a model that doesn't have a set of correct answers?\n",
        "\n",
        "Maybe *error* is not the right term, but we'll see how to define *metrics* to score and measure our unsupervised models.\n",
        "\n",
        "#### Unsupervised Clusterings:\n",
        "Since there are no correct labels, both of the following clusterings are valid!\n",
        "\n",
        "<img src=\"./imgs/clustering-00.jpg\" width=\"620px\"/>\n",
        "\n",
        "<img src=\"./imgs/clustering-01.jpg\" width=\"620px\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's run it !\n",
        "\n",
        "### Preparing Data\n",
        "\n",
        "We'll load the same wine dataset as last week and normalize its features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 1. Load Dataset\n",
        "WINE_FILE = \"https://raw.githubusercontent.com/DM-GY-9103-2024F-H/9103-utils/main/datasets/json/wines.json\"\n",
        "\n",
        "# Read into DataFrame\n",
        "wines_data = object_from_json_url(WINE_FILE)\n",
        "wines_df = pd.DataFrame.from_records(wines_data)\n",
        "\n",
        "## 3. Normalize\n",
        "wine_scaler = StandardScaler()\n",
        "wines_scaled = wine_scaler.fit_transform(wines_df)\n",
        "\n",
        "## 4. Select variables to be considered\n",
        "##    We're gonna drop the quality features to avoid re-clustering by quality\n",
        "features = wines_scaled.drop(columns=[\"quality\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clusterings\n",
        "\n",
        "Let's look at our first clustering algorithm:\n",
        "\n",
        "#### [K-means Clustering](https://scikit-learn.org/stable/modules/clustering.html#k-means):\n",
        "Tries to separate the data into $k$ groups with similar statistical properties. Requires the number of clusters to be determined beforehand, and the algorithm tries to minimize the difference between objects in a cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_clusters = 4\n",
        "\n",
        "## 5. Create Clustering object\n",
        "km_model = KMeansClustering(n_clusters=n_clusters)\n",
        "\n",
        "## 6. Run the model on the training data\n",
        "km_predicted = km_model.fit_predict(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plots\n",
        "\n",
        "Since we can't see in $4D$ or $5D$ yet, let's pick $2$ or $3$ variables to visualize our data and clusters.\n",
        "\n",
        "This could be any of our features, but let's look at the *covariances* related to the `quality` of the wine and pick the top $2$ or $3$ variables related to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Look at covariances again\n",
        "wines_scaled.cov()[\"quality\"].sort_values()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's plot `alcohol`, `chloride` and `density`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For plotting\n",
        "xl, yl, zl = \"alcohol\", \"chlorides\", \"density\"\n",
        "x = wines_scaled[xl]\n",
        "y = wines_scaled[yl]\n",
        "z = wines_scaled[zl]\n",
        "\n",
        "clusters = km_predicted[\"clusters\"]\n",
        "\n",
        "plt.scatter(x, z, c=clusters, marker='o', linestyle='', alpha=0.5)\n",
        "plt.title(\"k-means clustering\")\n",
        "plt.xlabel(xl)\n",
        "plt.ylabel(zl)\n",
        "plt.xlim(-2.2, 3.2)\n",
        "plt.ylim(-2.5, 3.5)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "\n",
        "ax.scatter(x, y, z, c=clusters, marker='o', linestyle='', alpha=0.5)\n",
        "\n",
        "ax.set_title(\"k-means clustering\")\n",
        "ax.set_xlabel(xl)\n",
        "ax.set_ylabel(yl)\n",
        "ax.set_zlabel(zl)\n",
        "\n",
        "ax.set_ylim(-2.5, 8)\n",
        "ax.set_zlim(-2.5, 2.5)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### More Clusterings!\n",
        "\n",
        "Let's look at another clustering method:\n",
        "\n",
        "#### [Gaussian Clustering](https://scikit-learn.org/stable/modules/mixture.html#mixture):\n",
        "This is similar to K-means, but this model assumes that all features of our data can be modeled as [Gaussian distributions](https://en.wikipedia.org/wiki/Normal_distribution).\n",
        "\n",
        "Repeat steps $5$ and $6$ for this clustering method.\n",
        "\n",
        "The Gaussian Clustering class is called `GaussianClustering` and its constructor takes the same parameters as the `KMeansClustering` constructor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Gaussian clustering\n",
        "\n",
        "## 5. Create Clustering object\n",
        "## 6. Run the model on the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot Gaussian clusters\n",
        "\n",
        "Just like the plots for `K-means`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: plot Gaussian Clustering results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### One More Method!\n",
        "\n",
        "Let's look at our final clustering method:\n",
        "\n",
        "#### [Spectral Clustering](https://scikit-learn.org/stable/modules/clustering.html#spectral-clustering):\n",
        "When appropriate, this method automatically combines and removes a few of our features, before doing K-means clustering. This should always be as good as, or better than, regular K-means clustering.\n",
        "\n",
        "The Spectral Clustering class is called `SpectralClustering` and its constructor takes the same parameters as the previous two methods.\n",
        "\n",
        "Repeat steps $5$ and $6$ to create a model and run it on our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Spectral clustering\n",
        "\n",
        "## 5. Create Clustering object\n",
        "## 6. Run the model on the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: plot Spectral Clustering results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scoring\n",
        "\n",
        "Would be nice to have a way to measure how good these clusters actually are.\n",
        "\n",
        "It would help determine if we need more clusters, or if one method is actually better than the others.\n",
        "\n",
        "There are a couple of ways to do this. We'll look at three of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Distance\n",
        "\n",
        "The first kind of scoring uses the distances between each point and its cluster's center as a metric.\n",
        "\n",
        "This is sometimes called the L2-distance, and it's just like the more familiar [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance) from geometry, but extended to measure more than just $2$ or $3$ dimensions/parameters.\n",
        "\n",
        "Each cluster's center is represented by the average values of all of the features of all of its members: $(\\overline{F_0}, \\overline{F_1}, \\overline{F_2}, ...)$ \n",
        "\n",
        "A smaller cluster distance means that the cluster center is a good representation of its members.\n",
        "\n",
        "Luckily, our clustering models have a `distance_error()` function that can be used to report the distance error, after `fit()` has been called."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "km_model.distance_error()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Repeat for the other $2$ methods\n",
        "\n",
        "Run the `distance_error()` function for the other methods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TODO: run the other method's distance_error()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Likelihood\n",
        "\n",
        "The second way of scoring clusters treats each cluster as a potential normal distribution, and then calculates the likelihood that each point came from its cluster distribution.\n",
        "\n",
        "Values closer to zero mean that the clusters' statistical properties (mean, variation) are good estimators for the data.\n",
        "\n",
        "Our model objects also have a `likelihood_error()` function we can use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "km_model.likelihood_error()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Repeat for the other methods\n",
        "\n",
        "Run the `likelihood_error()` function for the Gaussian and Spectral clustering models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: run likelihood_error() for other clustering methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Although somewhat related, the `distance` and `likelihood` calculations measure different things, and are in different units.\n",
        "\n",
        "We can't compare distances to likelihoods to draw any kind of conclusion.\n",
        "\n",
        "What we want to do is use either one of these metrics to select a clustering method and tune its parameters.\n",
        "\n",
        "### Balance\n",
        "\n",
        "A final metric we can consider when analyzing different clustering algorithms and strategies is to see how balanced the resulting clusters are. This isn't always important; we might have categories of items or events that are more common than others, and will produce unequal cluster groups.\n",
        "\n",
        "In other cases, where we know we want to have groups of similar sizes, this is a good metric to look at. For example, if we were to use the body measurement dataset for deciding how many sizes of bike helmets to produce, we should probably have sizes that cover similar portions of the population, and avoid very bespoke sizes that only fit few people.\n",
        "\n",
        "We compute `balance error` by summing the differences between our cluster sizes and the sizes of a perfectly balanced clustering. Once we have this sum, we scale it to get a number between $0$, for a perfectly balanced clustering, and $1$, for a most-unbalanced clustering.\n",
        "\n",
        "$\\displaystyle balance\\ error = \\frac{1}{2} \\left(\\frac{n}{n-1}\\right) \\sum_{i=1}^{n}{\\left|\\frac{C_i}{C_0 + C_1 + ... + C_n} - \\frac{1}{n}\\right|}$\n",
        "\n",
        "The $\\frac{C_i}{C_0 + C_1 + ... + C_n}$ terms are the sizes of our $n$ clusters expressed as the percentage of the total number of items in all clusters. The $\\frac{1}{n}$ term is the size of each cluster in a perfectly balanced clustering. We sum up these differences and scale it all by $\\frac{1}{2} \\left(\\frac{n}{n-1}\\right)$ to get a number between $0$ and $1$.\n",
        "\n",
        "We don't have to focus too much on this math right now. It's here for completeness and because it's good to practice reading an algorithm described as text, math equations and code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If we have a list of clusters, like this:\n",
        "print(\"Cluster List:\\n\", km_predicted[:10], \"\\n...\\n\")\n",
        "\n",
        "# This gives us the counts for each label:\n",
        "label_counts = km_predicted['clusters'].value_counts()\n",
        "print(\"Label Counts:\\n\", label_counts, \"\\n\")\n",
        "\n",
        "# This gives each cluster's size as a percentage of total number of items\n",
        "cluster_sizes_pct = label_counts / len(km_predicted['clusters'])\n",
        "print(\"Cluster Percents:\\n\", cluster_sizes_pct, \"\\n\")\n",
        "\n",
        "# This is the size of all clusters in a fully balanced clustering, expressed as a percentage\n",
        "balanced_cluster_size_pct = 1 / n_clusters\n",
        "print(\"Balanced Size:\", balanced_cluster_size_pct)\n",
        "\n",
        "# This is the sum of the distances between cluster sizes and perfectly-balanced sizes\n",
        "sum_distances = (cluster_sizes_pct - balanced_cluster_size_pct).abs().sum()\n",
        "print(\"Distance Sum:\", sum_distances)\n",
        "\n",
        "scale_factor = 0.5 * n_clusters / (n_clusters - 1)\n",
        "\n",
        "balance_error = scale_factor * sum_distances\n",
        "print(\"Balance Error:\", balance_error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Balance Error\n",
        "\n",
        "Luckily this has also been implemented for us and we can get our model's `balance error` by calling the `balance_error()` function of our clustering object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "km_model.balance_error()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Repeat for `Gaussian` and `Spectral` clustering\n",
        "\n",
        "Get `balance error` for all clustering methods by calling their `balance_error()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: get balance_error for gaussian and spectral clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Number of clusters\n",
        "\n",
        "If we consider the $3$ metrics for the $3$ methods, it seems like the `Spectral Clustering` algorithm performs a little bit better, even though it doesn't produce the most balanced clusters.\n",
        "\n",
        "Once we have chosen a method, we can tune its parameters to see if we can find a combination that produces \"better\" clusters.\n",
        "\n",
        "Since the only parameters our model has is the number of clusters, let's try different cluster numbers to see if there's a *better* way of clustering our wines:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# try 2 - 10 clusters\n",
        "num_clusters = list(range(2,10))\n",
        "\n",
        "# collect distance, likelihood and balance errors\n",
        "dist_err = []\n",
        "like_err = []\n",
        "bala_err = []\n",
        "\n",
        "# get distance, likelihood and balance for different clustering sizes\n",
        "for n in num_clusters:\n",
        "  mm = SpectralClustering(n_clusters=n)\n",
        "  mm.fit_predict(features)\n",
        "  dist_err.append(mm.distance_error())\n",
        "  like_err.append(mm.likelihood_error())\n",
        "  bala_err.append(mm.balance_error())\n",
        "\n",
        "\n",
        "# plot errors as function of number of clusters\n",
        "plt.plot(num_clusters, dist_err, marker='o')\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Distance Error\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(num_clusters, like_err, marker='o')\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Likelihood Error\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(num_clusters, bala_err, marker='o')\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Balance Error\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "Looks like $6$ could be a good number of clusters for this model, since adding additional clusters doesn't seem to make the errors go down that much.\n",
        "\n",
        "Let's look at our data and how it got clustered:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted = {}\n",
        "\n",
        "n = 6\n",
        "m_model = SpectralClustering(n_clusters=n)\n",
        "predicted = m_model.fit_predict(features)\n",
        "\n",
        "# For plotting\n",
        "xl, yl, zl = \"alcohol\", \"chlorides\", \"density\"\n",
        "x = wines_scaled[xl]\n",
        "y = wines_scaled[yl]\n",
        "z = wines_scaled[zl]\n",
        "\n",
        "plt.scatter(x, z, c=predicted[\"clusters\"], marker='o', linestyle='', alpha=0.5)\n",
        "plt.title(\"Spectral Clustering n = %s\" % n)\n",
        "plt.xlabel(xl)\n",
        "plt.ylabel(zl)\n",
        "plt.xlim(-2.2, 3.2)\n",
        "plt.ylim(-2.5, 3.5)\n",
        "plt.show()\n",
        "\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "\n",
        "ax.scatter(x, y, z, c=predicted, marker='o', linestyle='', alpha=0.5)\n",
        "plt.title(\"Spectral Clustering n = %s\" % n)\n",
        "ax.set_xlabel(xl)\n",
        "ax.set_ylabel(yl)\n",
        "ax.set_zlabel(zl)\n",
        "ax.set_ylim(-2.5, 8)\n",
        "ax.set_zlim(-2.5, 2.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis\n",
        "\n",
        "So, even though $6$ gives us the smallest error values, some of the clusters are really small and hard to find on the graphs.\n",
        "\n",
        "And the cluster sizes are really unequal.\n",
        "\n",
        "If this clustering is to be used for recommending wines to costumers, maybe using $4$ or $3$ clusters is a more sensible way of grouping our wines. Not because we have to balance the cluster sizes, but because the subtleties of having $6$ categories of wine might be less easy to explain.\n",
        "\n",
        "Using $4$ categories is probably more legible. The categories could be something like: `strong` for the more alcoholic wines, `bold` and `dense` for the ones that are less alcoholic, but have high density, and `wild` for the ones high in chlorides."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Images\n",
        "\n",
        "Clustering can also be used to help analyze image and audio files.\n",
        "\n",
        "We can extract color information from an image by clustering its pixels by their RGB values. This can be used to get an estimate of the most \"important\" colors in an image. They're not the most common colors, necessarily, but the colors necessary to represent the image.\n",
        "\n",
        "This is called `color quantization` and is a kind of compression because we reduce the total number of colors in an image from a possible $16,581,375$ unique colors to $4$, $8$, $16$, etc..., colors while preserving the overall appearance of the image. The cluster centers calculated become the color palette of the image.\n",
        "\n",
        "We'll start by loading an image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mimg = open_image(\"./data/arara.jpg\")\n",
        "\n",
        "display(mimg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Colors as features\n",
        "\n",
        "Since this process works on the pixels of a single image, we can think of each pixel as a record/measurement and its `R`, `G` and `B` values as its features.\n",
        "\n",
        "Before we put our image through clustering we should turn it into a `DataFrame` where the rows are the pixels and the columns are the `R`, `G`, `B` values.\n",
        "\n",
        "We can use the `pd.DataFrame.from_records()` function, but first we have to turn our pixel list into a list of objects:\n",
        "\n",
        "`[ [R,G,B], [R,G,B], [R,G,B], ... ]` -> `[{\"R\": R, \"G\": G, \"B\": B}, {\"R\": R, \"G\": G, \"B\": B}, ...]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: create list of objects from list of lists\n",
        "pixelObj = []\n",
        "\n",
        "img_df = pd.DataFrame.from_records(pixelObj)\n",
        "\n",
        "img_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoding/Scaling\n",
        "\n",
        "Not needed! All our features are already numbers, and they're all in the same units.\n",
        "\n",
        "### Cluster !\n",
        "\n",
        "Set up the `Clustering` object and run `fit_predtict()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_clusters=4\n",
        "\n",
        "## Create Clustering object\n",
        "km_model = KMeansClustering(n_clusters=n_clusters)\n",
        "\n",
        "## Run the model on the pixel data\n",
        "km_predicted = km_model.fit_predict(img_df)\n",
        "\n",
        "km_predicted.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Un-cluster\n",
        "\n",
        "The `km_predicted` variable holds a `DataFrame` that is a mapping from pixel index to cluster index, but what we really want is to re-map our original pixels into the color palette made up of our cluster centers.\n",
        "\n",
        "The `KMeansClustering` object has a member variable called `cluster_centers_` that holds the centers of our clusters. We can use this to build a new pixel array for our image.\n",
        "\n",
        "Let's take a look at the cluster centers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "km_model.cluster_centers_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have $4$ lists inside the `cluster_centers_` list. They should represent colors, but they're using floating point numbers right now, which will most likely give us troubles when we try to turn these into an image.\n",
        "\n",
        "Let's transform these into `int`s:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: use round() or int() to ensure our cluster centers are valid color values (ints)\n",
        "color_centers = []\n",
        "\n",
        "color_centers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can use these in a pixel array.\n",
        "\n",
        "Let's iterate through the `km_predicted[\"clusters\"]` and use those values to push their corresponding cluster center colors into a new pixel array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clustered_pxs = []\n",
        "\n",
        "# TODO: iterate through the cluster list and append the right color for each pixel\n",
        "\n",
        "display(make_image(clustered_pxs, mimg.size[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A few homeworks ago we looked at an image classification exercise and we saw that creating image filters by hand and selecting which features to look at can be a very laborious and complex process. Let's now combine what we learned about data normalization, distances, and learning algorithms to try to create a better image classification system.\n",
        "\n",
        "Consider the following image that is $4$ pixels wide by $4$ pixels high, it has a total of $16$ pixels:\n",
        "\n",
        "<img src=\"./imgs/pixdim-00.jpg\" height=\"200px\" />\n",
        "\n",
        "We can think of these $16$ pixels as the features of the image, and if we had a dataset of images of this size, we can think of the rows in this dataset as our different images and then the columns being the $16$ values for each of the pixels of those images.\n",
        "\n",
        "And the same is true for a dataset that has images that are $256$ by $256$ pixels:\n",
        "\n",
        "<img src=\"./imgs/pixdim-01.jpg\" height=\"200px\" />\n",
        "\n",
        "We would just have as many rows as we have images, and each of those rows would have $65,536$ columns/features, one for each of its pixels.\n",
        "\n",
        "We might be tempted now to develop some kind of method that uses $L2$ distances or cosine similarities to classify images by finding similarities between their pixel content. And that might work sometimes, but consider that computing the $L2$ distance between $2$ of our $256$ x $256$ pixel images would require squaring over $65000$ terms... and if we look at the black and white $4$ x $4$ images below, it's not so clear that subtracting raw pixel values would give us any indication of similarity:\n",
        "\n",
        "<img src=\"./imgs/pixdiff.jpg\" height=\"200px\" />\n",
        "\n",
        "The difference between the first image and the second image is of $8$ pixels. The difference between the first and third image is also of $8$ pixels, while the difference between the second and third image is of $16$ pixels, even though they're more similar. The same could be calculated for the last two pictures and the first.\n",
        "\n",
        "What we want to do instead is something called **_dimensionality reduction_**, where we turn those $65,000$ columns of pixel values into a smaller number of columns that represent more meaningful and complex information about the images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Projection\n",
        "\n",
        "One way of reducing the dimensions of our dataset is by doing something called **_feature projection_**, which is very simple to perform and understand.\n",
        "\n",
        "**_Feature Projection_** is when we just drop the columns in our dataset that we think are less important. The results of doing this can be seen in the following two images:\n",
        "\n",
        "<img src=\"./imgs/dimredproj-00.jpg\" height=\"350px\" />\n",
        "\n",
        "On the left side we have a graph of our data set in two dimensions. On the right, at the very bottom, we've projected all of those points straight down onto the x-axis by just ignoring their $y$ components.\n",
        "\n",
        "On next image we can see what the problem with doing that might be. Points that are actually further apart in our full dataset will seem closer once projected to a lower dimension like this, and points that are close together in $2D$ space can seem further apart in $1D$ space.\n",
        "\n",
        "<img src=\"./imgs/dimredproj-03.jpg\" height=\"350px\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Space-Filling Curves\n",
        "\n",
        "Another way of reducing dimensions is by using **_space-filling curves_**, like the [Hilbert curve](https://en.wikipedia.org/wiki/Hilbert_curve) or [Z-order curves](https://en.wikipedia.org/wiki/Z-order_curve).\n",
        "\n",
        "These functions reorder our data by tracing a curvy path between all the points in our dataset. If we look at the following images:\n",
        "\n",
        "<img src=\"./imgs/dimredcurve-00.jpg\" height=\"350px\" />\n",
        "\n",
        "The image on the left again shows our $2D$ data points graphed on a plane. On the right, we've drawn a line that connects all of our points by starting with a point on the lower-left side of the graph and then connecting to the nearest point, and then the next nearest point and so on and so on. In the end we have the light-blue line that connects and re-orders all of our $2D$ points into a $1D$ line.\n",
        "\n",
        "The problem with this method is that **_space-filling curves_** still have some discontinuities, or jumps, which might result in the same problem as **_projection_**, where closer points become further apart after transformation and vice-versa.\n",
        "\n",
        "### Principal Component Analysis\n",
        "\n",
        "One final way of reducing the dimensions in our dataset is by doing something called **_principal component analysis_** or **_PCA_**. This is a kind of unsupervised learning algorithm that looks at all of our original data and tries to figure out the directions in which it spreads out the most.\n",
        "\n",
        "<img src=\"./imgs/dimredpca-00.jpg\" height=\"350px\" />\n",
        "\n",
        "Again, on the left we have the original $2D$ data on a plane. On the right we see the original data with a line that represents the direction in which the data seems to spread.\n",
        "\n",
        "**_PCA_** is kind of, sort of, like doing **_Linear Regression_** in a special manner, and finding many lines-of-best-fit, one for each of our features. We can think of these new lines as new axes that we re-orient our data around, and instead of having the $x$ axis and the $y$ axis we'll have $2$ new axes, one in the direction that captures the most variation in our data, and another in a perpendicular direction, that captures less of the variation in our data, like below:\n",
        "\n",
        "<img src=\"./imgs/dimredpca-03.jpg\" height=\"350px\" />\n",
        "\n",
        "But, wait ! If we have $N$ dimensions to begin with and we end up with these $N$ lines/axis/features, isn't it the same as before ? We still have $N$ dimensions to deal with!\n",
        "\n",
        "Yes, but the thing about the **_PCA_** axes is that these lines, or, principal components, are actually ordered by importance, so the very first principal component is the one that captures most of the variation in our data, the second one captures the second most amount of variation in our data, the third one the third most, and so on and so on...\n",
        "\n",
        "What we want to do after we compute the principal components of our dataset is perform a **_projection_** and chop off most of the new dimensions. Most of the information about our data will be captured by the first couple of principal components, so we can now project our data onto a few of these new axes and they'll retain most of the information about our data.\n",
        "\n",
        "<img src=\"./imgs/dimredpca-02.jpg\" height=\"350px\" />\n",
        "\n",
        "In the $2D$ case above if we only consider the first principal component and project our data onto that blue line we retain most of the information about the relative distances between the points in our dataset, but since they're all projected onto a line, they can be represented by just $1$ feature/value. In the end we were able to reduce our data from having $2$ variables in the $XY$ plane to just having one variable in the direction of the first principal component.\n",
        "\n",
        "Let's look at some examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### PCA in 2D\n",
        "\n",
        "Let's reload the `ANSUR` data and look at only the features about hands. We'll use PCA to reduce the dimension of this subset of our data from $3$ to $1$ dimensions.\n",
        "\n",
        "First, let's only consider `hand.breadth` and `hand.length`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# scale and pca objects\n",
        "mss = StandardScaler()\n",
        "pca = PCA(n_components=1)\n",
        "\n",
        "ANSUR_FILE = \"https://raw.githubusercontent.com/DM-GY-9103-2024F-H/9103-utils/main/datasets/json/ansur.json\"\n",
        "ansur_data = object_from_json_url(ANSUR_FILE)\n",
        "\n",
        "ansur_df = pd.json_normalize(ansur_data)\n",
        "ansur_df.head()\n",
        "\n",
        "# drop all other features\n",
        "ansur_df = ansur_df[[\"hand.breadth\", \"hand.length\"]]\n",
        "\n",
        "# perform scaling and pca\n",
        "ansur_df = mss.fit_transform(ansur_df)\n",
        "ansur_pca_df = pca.fit_transform(ansur_df)\n",
        "\n",
        "# project from 1 PCA back to original 2 dimensions\n",
        "ansur_pca_i_df = pca.inverse_transform(ansur_pca_df)\n",
        "\n",
        "# plot\n",
        "plt.scatter(ansur_df[\"hand.breadth\"], ansur_df[\"hand.length\"], s=2)\n",
        "plt.scatter(ansur_df[\"hand.breadth\"], [-3.5]*len(ansur_df[\"hand.length\"]), s=2, c='#ff8000')\n",
        "plt.scatter(ansur_pca_i_df[\"hand.breadth\"], ansur_pca_i_df[\"hand.length\"], s=2, c='r')\n",
        "plt.xlim(-4, 4)\n",
        "plt.ylim(-4, 4)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The graph above shows the original data for the `hand.breadth` and `hand.length` features in blue.\n",
        "\n",
        "The orange points show the data projected onto the `hand.breadth` axis. It is $1$ dimensional, but we've completely ignored the `hand.length` feature.\n",
        "\n",
        "The red points show our data projected onto the first principal component.\n",
        "\n",
        "If we call the `pca.explained_variance()` function we can see how much of our data's information has been retained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TODO: see how much of the information from our data has been retained after PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### PCA in 3D\n",
        "\n",
        "We'll now repeat the same process, but with the addition of the `hand.palm` feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ansur_df = pd.json_normalize(ansur_data)\n",
        "ansur_df.head()\n",
        "\n",
        "ansur_df = ansur_df[[\"hand.breadth\", \"hand.length\", \"hand.palm\"]]\n",
        "\n",
        "# perform scaling and pca\n",
        "ansur_df = mss.fit_transform(ansur_df)\n",
        "ansur_pca_df = pca.fit_transform(ansur_df)\n",
        "\n",
        "# project from 1 PCA back to original 2 dimensions\n",
        "ansur_pca_i_df = pca.inverse_transform(ansur_pca_df)\n",
        "\n",
        "# plot\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "ax.scatter(ansur_df[\"hand.breadth\"], ansur_df[\"hand.length\"], ansur_df[\"hand.palm\"], s=3, alpha=0.15)\n",
        "ax.scatter(ansur_pca_i_df[\"hand.breadth\"], ansur_pca_i_df[\"hand.length\"], ansur_pca_i_df[\"hand.palm\"], s=3, c='r', alpha=0.333)\n",
        "ax.set_xlim((-4, 4))\n",
        "ax.set_ylim((-4, 4))\n",
        "ax.set_zlim((-4, 4))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TODO: how much of the original data information did we keep ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### PCA in all dimensions\n",
        "\n",
        "What if we reduce all of our features to just $1$? How much of the data's information is kept?\n",
        "\n",
        "If we want to keep $80\\%$ of our data's variance, how many components should our PCA use?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TODO: repeat above process, using all of the original features, except gender\n",
        "\n",
        "## TODO: get explained_variance() for n_components=1\n",
        "\n",
        "## TODO: get n_components for explained_variance >= 0.80"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### More PCA in Multiple Dimensions\n",
        "\n",
        "The above exercises are good for us to start building intuition about PCA, but even though we can perform PCA and reduce our features from $15$ to $4$, $5$ or $6$ principal components, it gets hard to visualize the effects of these transformations.\n",
        "\n",
        "Let's use images. We'll load images with about $10000$ pixels/dimensions, and perform PCA using only $10$ components. Then we'll remap our $10$ components for each image back to $10000$ pixels to look at the effects of **_compressing_** our data like this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### PCA on Images\n",
        "\n",
        "Let's load all of the images inside the `data/imgs/att-faces/` directory. These images are in sub-directories that specify the `id` of the person in the image: `s0` is `subject 0`, `s1` is `subject 1`, and so on.\n",
        "\n",
        "We'll keep track of these ids and create numeric labels for them as we read the data from the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# lists for keeping track of image pixel lists, subject numeric id and subject label\n",
        "face_pixels = []\n",
        "face_ids = []\n",
        "id2label = []\n",
        "\n",
        "# 40 directories\n",
        "for l in range(1, 41):\n",
        "  id2label.append(f\"s{l}\")\n",
        "  # 10 images per directory\n",
        "  for i in range(1, 11):\n",
        "    mimg = open_image(f\"./data/imgs/att-faces/s{l}/{i}.pgm\")\n",
        "    face_pixels.append(mimg.pixels)\n",
        "    face_ids.append(l)\n",
        "\n",
        "# display first image\n",
        "display(make_image(face_pixels[0], width=92))\n",
        "\n",
        "# print len of lists to make sure sizes match\n",
        "len(face_pixels), len(face_ids),\\\n",
        "\n",
        "# check number of labels matches number of directories\n",
        "len(id2label),\\\n",
        "\n",
        "# check how many pixels per image and look at first 10 pixel values\n",
        "len(face_pixels[0]), face_pixels[0][:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These look ok.\n",
        "\n",
        "#### Run PCA\n",
        "\n",
        "We can run PCA directly on these lists of pixels. We don't even have to scale the data because we know the pixel values are all between $0$ and $255$ for greyscale images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# run pca and get first 10 PCs\n",
        "pca = PCA(n_components=10)\n",
        "faces_df = pca.fit_transform(face_pixels)\n",
        "\n",
        "# put id values in DataFrame\n",
        "faces_df[\"id\"] = face_ids\n",
        "\n",
        "print(pca.explained_variance())\n",
        "faces_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Interpretation\n",
        "\n",
        "Our PCA-transformed data retained about $60\\%$ of the information from the original data.\n",
        "\n",
        "Our `DataFrame` now is expressed in terms of the first $10$ principal components of our data.\n",
        "\n",
        "That means we were able to compress our data and reduce the number of dimensions for each of our images from $10304$ to $10$ ! That's a reduction factor of $1000$ ! We could start to think about doing clustering or classification by measuring distances between these images because now they only have $10$ dimensions.\n",
        "\n",
        "But, let's visualize our new compressed images first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# reconstruction\n",
        "pca_pixels = pca.inverse_transform(faces_df)\n",
        "\n",
        "display(make_image(face_pixels[0], width=92))\n",
        "display(make_image(pca_pixels.loc[0], width=92))\n",
        "\n",
        "display(make_image(face_pixels[110], width=92))\n",
        "display(make_image(pca_pixels.loc[110], width=92))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ü§î\n",
        "\n",
        "These don't look too good at first, but they also don't look that bad. Given how regular all of the images are, maybe this level of detail is all that is needed to do any kind of classification or clustering.\n",
        "\n",
        "#### More components\n",
        "\n",
        "Re-run PCA with $20$ and $120$ components. Don't worry about printing the `DataFrame`, but reconstruct some of the images like we did in the cell above.\n",
        "\n",
        "What is the effect of keeping more components in our PCA?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TODO: reconstruct images using 20 and 120 component PCAs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Back to $10$ components\n",
        "\n",
        "Let's go back to just using $10$ PCs and see how the first $100$ images are distributed in the first $2$ and $3$ dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# run pca and get first 10 PCs\n",
        "pca = PCA(n_components=10)\n",
        "faces_df = pca.fit_transform(face_pixels)\n",
        "\n",
        "# put id values in DataFrame\n",
        "faces_df[\"id\"] = face_ids\n",
        "\n",
        "# first 10 ids\n",
        "x = faces_df[\"PC0\"][:100]\n",
        "y = faces_df[\"PC1\"][:100]\n",
        "z = faces_df[\"PC2\"][:100]\n",
        "c = faces_df[\"id\"][:100]\n",
        "\n",
        "plt.scatter(x, y, c=c, marker='o', linestyle='', alpha=1, cmap=\"tab10\")\n",
        "plt.title(\"Principal Components\")\n",
        "plt.xlabel(\"PC 0\")\n",
        "plt.ylabel(\"PC 1\")\n",
        "plt.show()\n",
        "\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "ax.scatter(x,y,z, c=c, marker='o', linestyle='', alpha=1, cmap=\"tab10\")\n",
        "ax.set_title(\"Principal Components\")\n",
        "ax.set_xlabel(\"PC 0\")\n",
        "ax.set_ylabel(\"PC 1\")\n",
        "ax.set_zlabel(\"PC 2\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Interpretation\n",
        "\n",
        "Here are only looking at the top $2$ and $3$ dimensions of our data and we can already see some patterns/clusters.\n",
        "\n",
        "The colors were added manually using the correct `id` for each image, so that helps to see patterns...\n",
        "\n",
        "So, let's run the `RandomForestClassifier` algorithm on this dataset to see if PCA dimensionality reduction can actually help prepare image datasets for classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PCA + Classification\n",
        "\n",
        "We have already prepared our `DataFrame` and added a column with the numerical `id` of each image.\n",
        "\n",
        "We can check by running `faces_df.head()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "faces_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Separate Train and Test Data\n",
        "\n",
        "In order to measure how well our classifier works in general, we should separate our data into $2$ subsets, one which will get used to train our model, and another that we can use to measure how well our model performs on data it hasn't seen before.\n",
        "\n",
        "This is to avoid having our model memorize the training data, and then performing horrible on new data in the future.\n",
        "\n",
        "We'll use the `Scikit-Learn` function `train_test_split` to split our `DataFrame` into $2$ equal datasets.\n",
        "\n",
        "Since the split is random, the `while` loop below is just to guarantee that our test dataset has images from all subjects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_ids = []\n",
        "\n",
        "while len(set(test_ids)) != len(id2label):\n",
        "  faces_train_df, faces_test_df = train_test_split(faces_df, test_size=0.5)\n",
        "  test_ids = faces_test_df[\"id\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train Classifier\n",
        "\n",
        "The below steps should look familiar.\n",
        "\n",
        "We split the output feature from he rest of the features, and pass those to the `fit()` function of an instance of the `RandomForestClassifier` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "face_model = RandomForestClassifier()\n",
        "\n",
        "train_features = faces_train_df.drop(columns=[\"id\"])\n",
        "train_ids = faces_train_df[\"id\"]\n",
        "\n",
        "# Create a model that classifies faces based on principal components\n",
        "face_model.fit(train_features, train_ids)\n",
        "\n",
        "## 6. Run the model on the training data\n",
        "train_predicted = face_model.predict(train_features)\n",
        "\n",
        "## 7. Measure error\n",
        "classification_error(train_ids, train_predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Classification Error should be close to $0$.\n",
        "\n",
        "#### Test Data\n",
        "\n",
        "Now we'll run the `predict()` function of our classifier on our test dataset and see how it performs.\n",
        "\n",
        "Again, this should look familiar: we just have to remember to separate the output feature (`id`) from the other features, and then pass those $2$ lists to the `predict()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_features = faces_test_df.drop(columns=[\"id\"])\n",
        "test_ids = faces_test_df[\"id\"]\n",
        "\n",
        "## 6. Run the model on the test data\n",
        "test_predicted = face_model.predict(test_features)\n",
        "\n",
        "## 7. Measure error\n",
        "classification_error(test_ids, test_predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Interpretation\n",
        "\n",
        "The Classification Error should be around $20\\%$. This isn't bad !\n",
        "\n",
        "Remember, we just loaded up some unknown dataset of images, automatically reduced the number of dimensions in each image from $10304$ to $10$ and used a basic `RandomForestClassifier()` to learn patterns on the compressed data.\n",
        "\n",
        "We didn't have to look at pixels, we didn't have to manually filter images, we didn't even have to guess which features to keep. PCA helped with all of this.\n",
        "\n",
        "#### Confusion Matrix\n",
        "\n",
        "We can run the following cell to print a **_Confusion Matrix_** for our model. This is a graph that shows how well our model performed on each class of our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display_confusion_matrix(test_ids, test_predicted, display_labels=id2label)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There's probably a class or two that our model didn't do too well with.\n",
        "\n",
        "#### Improving Our Model\n",
        "\n",
        "How can we improve this model ? We picked $10$ components just to see what would happens with a dimensionality reduction of $1000$.\n",
        "\n",
        "What happens if we use $100$ components? $100$ is still a lot less than $10304$ and probably still easy for our `RandomForestClassifier` to handle.\n",
        "\n",
        "How much of the data's variance is kept with $100$ components?\n",
        "\n",
        "In order to improve the classifier it might not be enough to just increase the number of components, but also adjust the train/test split. In order for the model to really learn more complex pattern we should give it a bit more data to train on.\n",
        "\n",
        "Maybe decreasing the `test_size` parameter in the `train_test_split()` function to $0.3$ or $0.25$ will help the model learn from the extra information in the $100$ components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TODO: Re-run the classification training, but using 100 PCs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusion\n",
        "\n",
        "Unsupervised learning can be a very powerful and useful tool for performing exploratory data analysis and creating recommendation systems.\n",
        "\n",
        "Because there's no labeled/correct answer in unsupervised learning, we can be a bit more subjective in how we pick our metrics for success."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPxe2qYxIG7EblrvD1C4Pmv",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.17 ('hf-model')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "89e384cab7c47fb35ec95d2248b519cf922ee174880eed636c26cdfb6c4df768"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
