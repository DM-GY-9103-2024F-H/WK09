{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QshK8s21WBrf"
      },
      "source": [
        "# Week 07\n",
        "\n",
        "Unsupervised Learning: Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hf8SXUwWOho"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Run the following 2 cells to import all necessary libraries and helpers for this week's exercises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget -q https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/src/data_utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from data_utils import StandardScaler\n",
        "from data_utils import KMeansClustering, GaussianClustering, SpectralClustering\n",
        "from data_utils import object_from_json_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## More Wine ! üç∑üç∑üç∑\n",
        "\n",
        "Let's pretend we own an online wine store.\n",
        "\n",
        "Last week we created a model that predicts wine quality based on a bunch of its properties. We could use this model to figure out how much to pay suppliers for the wine, and how much to charge costumers.\n",
        "\n",
        "But, maybe this \"`quality`\" feature might not be something we want to share with our costumers. Even though it's based on data, it sounds abstract and subjective and would require explanations about our data and our process, which could create confusion.\n",
        "\n",
        "Using all six features from the original dataset (`alcohol`, `acidity`, `density`, etc) might also not be very useful for costumers who want to buy new wines that are similar to ones that they have previously liked.\n",
        "\n",
        "What we can do instead is classify the wines into groups that take into account all of the features of the dataset, but present costumers with a more manageable amount of information.\n",
        "\n",
        "### Recommendations\n",
        "\n",
        "What we're really hoping to have is a simple recommendation system for our costumers, where we can recommend wines based on previous wines they liked, without them having to know the $6$ features of the previous wines.\n",
        "\n",
        "There are a few ways of doing this, but the strategy we'll take is called clustering.\n",
        "\n",
        "### Clustering\n",
        "\n",
        "[Clustering](https://en.wikipedia.org/wiki/Cluster_analysis), or cluster analysis, is an example of an *unsupervised* learning method that groups items based on their many features and properties.\n",
        "\n",
        "We'll use it to divide our wines in such a way that wines in the same group, or *cluster*, are more similar to each other than to wines in other clusters.\n",
        "\n",
        "These clusters won't necessarily correlate directly to the features in our dataset, but will be computed using a combination of the features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Supervised Learning\n",
        "\n",
        "The models that we've trained so far for doing regression and classification are considered *supervised* models. During training we give the model our input features, but also provide it with the *correct* values for the output signals. These output signals tend to be human-labeled values, and are sometimes called the *supervisory signals*.\n",
        "\n",
        "When fully-labeled training data is processed during training, we are hoping that the model learns to extrapolate what it *sees* in the labeled data to new, unseen, unlabeled instances of data with the same input features, but unknown output values.\n",
        "\n",
        "#### Supervised Classification:\n",
        "\n",
        "Given a set of initial data points with labels:<br>\n",
        "<img src=\"./imgs/classification-02.jpg\" width=\"620px\"/>\n",
        "\n",
        "We create a model that learns to assign labels to the original points:\n",
        "<img src=\"./imgs/classification-03.jpg\" width=\"620px\"/>\n",
        "\n",
        "so that later we can assign correct labels to new data points:\n",
        "<img src=\"./imgs/classification-04.jpg\" width=\"620px\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unsupervised Learning\n",
        "\n",
        "Unlike supervised learning, unsupervised models learn patterns from unlabeled data. This means all of the features are considered input features, and there are no separate output features or signals. The idea is that by analyzing and processing data in specific ways, the model is able to build a concise representation of its features and create new ways of interpreting, visualizing or generating similar data.\n",
        "\n",
        "We can use unsupervised learning models to explore new datasets and try to simplify our data before we do any kind of supervised learning.\n",
        "\n",
        "We can also use supervised learning to build recommendation systems that learn how to group items by their many features or characteristics.\n",
        "\n",
        "The steps for training an unsupervised model should seem familiar:\n",
        "\n",
        "1. Load dataset\n",
        "2. Encode label features as numbers\n",
        "3. Normalize the data\n",
        "4. Select variables and features to be considered\n",
        "5. Create a model\n",
        "6. Run model on input data and test data\n",
        "7. Measure error\n",
        "\n",
        "Even though it all looks familiar, that last step isn't very obvious.\n",
        "\n",
        "How do we measure error on a model that doesn't have a set of correct answers?\n",
        "\n",
        "Maybe *error* is not the right term, but we'll see how to define *metrics* to score and measure our unsupervised models.\n",
        "\n",
        "#### Unsupervised Clusterings:\n",
        "Since there are no correct labels, both of the following clusterings are valid!\n",
        "\n",
        "<img src=\"./imgs/clustering-00.jpg\" width=\"620px\"/>\n",
        "\n",
        "<img src=\"./imgs/clustering-01.jpg\" width=\"620px\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's run it !\n",
        "\n",
        "### Preparing Data\n",
        "\n",
        "We'll load the same wine dataset as last week and normalize its features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 1. Load Dataset\n",
        "WINE_FILE = \"https://raw.githubusercontent.com/DM-GY-9103-2024F-H/9103-utils/main/datasets/json/wines.json\"\n",
        "\n",
        "# Read into DataFrame\n",
        "wines_data = object_from_json_url(WINE_FILE)\n",
        "wines_df = pd.DataFrame.from_records(wines_data)\n",
        "\n",
        "## 3. Normalize\n",
        "wine_scaler = StandardScaler()\n",
        "wines_scaled = wine_scaler.fit_transform(wines_df)\n",
        "\n",
        "## 4. Select variables to be considered\n",
        "##    We're gonna drop the quality features to avoid re-clustering by quality\n",
        "features = wines_scaled.drop(columns=[\"quality\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clusterings\n",
        "\n",
        "Let's look at our first clustering algorithm:\n",
        "\n",
        "#### [K-means Clustering](https://scikit-learn.org/stable/modules/clustering.html#k-means):\n",
        "Tries to separate the data into $k$ groups with similar statistical properties. Requires the number of clusters to be determined beforehand, and the algorithm tries to minimize the difference between objects in a cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_clusters = 4\n",
        "\n",
        "## 5. Create Clustering object\n",
        "km_model = KMeansClustering(n_clusters=n_clusters)\n",
        "\n",
        "## 6. Run the model on the training data\n",
        "km_predicted = km_model.fit_predict(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plots\n",
        "\n",
        "Since we can't see in $4D$ or $5D$ yet, let's pick $2$ or $3$ variables to visualize our data and clusters.\n",
        "\n",
        "This could be any of our features, but let's look at the *covariances* related to the `quality` of the wine and pick the top $2$ or $3$ variables related to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Look at covariances again\n",
        "wines_scaled.cov()[\"quality\"].sort_values()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's plot `alcohol`, `chloride` and `density`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For plotting\n",
        "xl, yl, zl = \"alcohol\", \"chlorides\", \"density\"\n",
        "x = wines_scaled[xl]\n",
        "y = wines_scaled[yl]\n",
        "z = wines_scaled[zl]\n",
        "\n",
        "clusters = km_predicted[\"clusters\"]\n",
        "\n",
        "plt.scatter(x, z, c=clusters, marker='o', linestyle='', alpha=0.5)\n",
        "plt.title(\"k-means clustering\")\n",
        "plt.xlabel(xl)\n",
        "plt.ylabel(zl)\n",
        "plt.xlim(-2.2, 3.2)\n",
        "plt.ylim(-2.5, 3.5)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "\n",
        "ax.scatter(x, y, z, c=clusters, marker='o', linestyle='', alpha=0.5)\n",
        "\n",
        "ax.set_title(\"k-means clustering\")\n",
        "ax.set_xlabel(xl)\n",
        "ax.set_ylabel(yl)\n",
        "ax.set_zlabel(zl)\n",
        "\n",
        "ax.set_ylim(-2.5, 8)\n",
        "ax.set_zlim(-2.5, 2.5)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### More Clusterings!\n",
        "\n",
        "Let's look at another clustering method:\n",
        "\n",
        "#### [Gaussian Clustering](https://scikit-learn.org/stable/modules/mixture.html#mixture):\n",
        "This is similar to K-means, but this model assumes that all features of our data can be modeled as [Gaussian distributions](https://en.wikipedia.org/wiki/Normal_distribution).\n",
        "\n",
        "Repeat steps $5$ and $6$ for this clustering method.\n",
        "\n",
        "The Gaussian Clustering class is called `GaussianClustering` and its constructor takes the same parameters as the `KMeansClustering` constructor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Gaussian clustering\n",
        "\n",
        "## 5. Create Clustering object\n",
        "## 6. Run the model on the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot Gaussian clusters\n",
        "\n",
        "Just like the plots for `K-means`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: plot Gaussian Clustering results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### One More Method!\n",
        "\n",
        "Let's look at our final clustering method:\n",
        "\n",
        "#### [Spectral Clustering](https://scikit-learn.org/stable/modules/clustering.html#spectral-clustering):\n",
        "When appropriate, this method automatically combines and removes a few of our features, before doing K-means clustering. This should always be as good as, or better than, regular K-means clustering.\n",
        "\n",
        "The Spectral Clustering class is called `SpectralClustering` and its constructor takes the same parameters as the previous two methods.\n",
        "\n",
        "Repeat steps $5$ and $6$ to create a model and run it on our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Spectral clustering\n",
        "\n",
        "## 5. Create Clustering object\n",
        "## 6. Run the model on the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: plot Spectral Clustering results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scoring\n",
        "\n",
        "Would be nice to have a way to measure how good these clusters actually are.\n",
        "\n",
        "It would help determine if we need more clusters, or if one method is actually better than the others.\n",
        "\n",
        "There are a couple of ways to do this. We'll look at three of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Distance\n",
        "\n",
        "The first kind of scoring uses the distances between each point and its cluster's center as a metric.\n",
        "\n",
        "This is sometimes called the L2-distance, and it's just like the more familiar [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance) from geometry, but extended to measure more than just $2$ or $3$ dimensions/parameters.\n",
        "\n",
        "Each cluster's center is represented by the average values of all of the features of all of its members: $(\\overline{F_0}, \\overline{F_1}, \\overline{F_2}, ...)$ \n",
        "\n",
        "A smaller cluster distance means that the cluster center is a good representation of its members.\n",
        "\n",
        "Luckily, our clustering models have a `distance_error()` function that can be used to report the distance error, after `fit()` has been called."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "km_model.distance_error()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Repeat for the other $2$ methods\n",
        "\n",
        "Run the `distance_error()` function for the other methods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TODO: run the other method's distance_error()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Likelihood\n",
        "\n",
        "The second way of scoring clusters treats each cluster as a potential normal distribution, and then calculates the likelihood that each point came from its cluster distribution.\n",
        "\n",
        "Values closer to zero mean that the clusters' statistical properties (mean, variation) are good estimators for the data.\n",
        "\n",
        "Our model objects also have a `likelihood_error()` function we can use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "km_model.likelihood_error()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Repeat for the other methods\n",
        "\n",
        "Run the `likelihood_error()` function for the Gaussian and Spectral clustering models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: run likelihood_error() for other clustering methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Although somewhat related, the `distance` and `likelihood` calculations measure different things, and are in different units.\n",
        "\n",
        "We can't compare distances to likelihoods to draw any kind of conclusion.\n",
        "\n",
        "What we want to do is use either one of these metrics to select a clustering method and tune its parameters.\n",
        "\n",
        "### Balance\n",
        "\n",
        "A final metric we can consider when analyzing different clustering algorithms and strategies is to see how balanced the resulting clusters are. This isn't always important; we might have categories of items or events that are more common than others, and will produce unequal cluster groups.\n",
        "\n",
        "In other cases, where we know we want to have groups of similar sizes, this is a good metric to look at. For example, if we were to use the body measurement dataset for deciding how many sizes of bike helmets to produce, we should probably have sizes that cover similar portions of the population, and avoid very bespoke sizes that only fit few people.\n",
        "\n",
        "We compute `balance error` by summing the differences between our cluster sizes and the sizes of a perfectly balanced clustering. Once we have this sum, we scale it to get a number between $0$, for a perfectly balanced clustering, and $1$, for a most-unbalanced clustering.\n",
        "\n",
        "$\\displaystyle balance\\ error = \\frac{1}{2} \\left(\\frac{n}{n-1}\\right) \\sum_{i=1}^{n}{\\left|\\frac{C_i}{C_0 + C_1 + ... + C_n} - \\frac{1}{n}\\right|}$\n",
        "\n",
        "The $\\frac{C_i}{C_0 + C_1 + ... + C_n}$ terms are the sizes of our $n$ clusters expressed as the percentage of the total number of items in all clusters. The $\\frac{1}{n}$ term is the size of each cluster in a perfectly balanced clustering. We sum up these differences and scale it all by $\\frac{1}{2} \\left(\\frac{n}{n-1}\\right)$ to get a number between $0$ and $1$.\n",
        "\n",
        "We don't have to focus too much on this math right now. It's here for completeness and because it's good to practice reading an algorithm described as text, math equations and code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If we have a list of clusters, like this:\n",
        "print(km_predicted[:10], \"\\n...\")\n",
        "\n",
        "# This gives us the counts for each label:\n",
        "label_counts = km_predicted['clusters'].value_counts()\n",
        "print(label_counts)\n",
        "\n",
        "# This gives each cluster's size as a percentage of total number of items\n",
        "cluster_sizes_pct = label_counts / len(km_predicted['clusters'])\n",
        "print(cluster_sizes_pct)\n",
        "\n",
        "# This is the size of all clusters in a fully balanced clustering, expressed as a percentage\n",
        "balanced_cluster_size_pct = 1 / n_clusters\n",
        "print(balanced_cluster_size_pct)\n",
        "\n",
        "# This is the sum of the distances between cluster sizes and perfectly-balanced sizes\n",
        "sum_distances = (cluster_sizes_pct - balanced_cluster_size_pct).abs().sum()\n",
        "print(sum_distances)\n",
        "\n",
        "scale_factor = 0.5 * n_clusters / (n_clusters - 1)\n",
        "\n",
        "balance_error = scale_factor * sum_distances\n",
        "print(balance_error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Balance Error\n",
        "\n",
        "Luckily this has also been implemented for us and we can get our model's `balance error` by calling the `balance_error()` function of our clustering object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "km_model.balance_error()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Repeat for `Gaussian` and `Spectral` clustering\n",
        "\n",
        "Get `balance error` for all clustering methods by calling their `balance_error()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: get balance_error for gaussian and spectral clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Number of clusters\n",
        "\n",
        "If we consider the $3$ metrics for the $3$ methods, it seems like the `Spectral Clustering` algorithm performs a little bit better, even though it doesn't produce the most balanced clusters.\n",
        "\n",
        "Once we have chosen a method, we can tune its parameters to see if we can find a combination that produces \"better\" clusters.\n",
        "\n",
        "Since the only parameters our model has is the number of clusters, let's try different cluster numbers to see if there's a *better* way of clustering our wines:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# try 2 - 10 clusters\n",
        "num_clusters = list(range(2,10))\n",
        "\n",
        "# collect distance, likelihood and balance errors\n",
        "dist_err = []\n",
        "like_err = []\n",
        "bala_err = []\n",
        "\n",
        "# get distance, likelihood and balance for different clustering sizes\n",
        "for n in num_clusters:\n",
        "  mm = SpectralClustering(n_clusters=n)\n",
        "  mm.fit_predict(features)\n",
        "  dist_err.append(mm.distance_error())\n",
        "  like_err.append(mm.likelihood_error())\n",
        "  bala_err.append(mm.balance_error())\n",
        "\n",
        "\n",
        "# plot errors as function of number of clusters\n",
        "plt.plot(num_clusters, dist_err, marker='o')\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Distance Error\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(num_clusters, like_err, marker='o')\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Likelihood Error\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(num_clusters, bala_err, marker='o')\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Balance Error\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "Looks like $6$ could be a good number of clusters for this model, since adding additional clusters doesn't seem to make the errors go down that much.\n",
        "\n",
        "Let's look at our data and how it got clustered:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted = {}\n",
        "\n",
        "n = 6\n",
        "m_model = SpectralClustering(n_clusters=n)\n",
        "predicted = m_model.fit_predict(features)\n",
        "\n",
        "# For plotting\n",
        "xl, yl, zl = \"alcohol\", \"chlorides\", \"density\"\n",
        "x = wines_scaled[xl]\n",
        "y = wines_scaled[yl]\n",
        "z = wines_scaled[zl]\n",
        "\n",
        "plt.scatter(x, z, c=predicted[\"clusters\"], marker='o', linestyle='', alpha=0.5)\n",
        "plt.title(\"Spectral Clustering n = %s\" % n)\n",
        "plt.xlabel(xl)\n",
        "plt.ylabel(zl)\n",
        "plt.xlim(-2.2, 3.2)\n",
        "plt.ylim(-2.5, 3.5)\n",
        "plt.show()\n",
        "\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "\n",
        "ax.scatter(x, y, z, c=predicted, marker='o', linestyle='', alpha=0.5)\n",
        "plt.title(\"Spectral Clustering n = %s\" % n)\n",
        "ax.set_xlabel(xl)\n",
        "ax.set_ylabel(yl)\n",
        "ax.set_zlabel(zl)\n",
        "ax.set_ylim(-2.5, 8)\n",
        "ax.set_zlim(-2.5, 2.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis\n",
        "\n",
        "So, even though $6$ gives us the smallest error values, some of the clusters are really small and hard to find on the graphs.\n",
        "\n",
        "And the cluster sizes are really unequal.\n",
        "\n",
        "If this clustering is to be used for recommending wines to costumers, maybe using $4$ or $3$ clusters is a more sensible way of grouping our wines. Not because we have to balance the cluster sizes, but because the subtleties of having $6$ categories of wine might be less easy to explain.\n",
        "\n",
        "Using $4$ categories is probably more legible. The categories could be something like: `strong` for the more alcoholic wines, `bold` and `dense` for the ones that are less alcoholic, but have high density, and `wild` for the ones high in chlorides.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Unsupervised learning can be a very powerful and useful tool for performing exploratory data analysis and creating recommendation systems.\n",
        "\n",
        "Because there's no labeled/correct answer in unsupervised learning, we can be a bit more subjective in how we pick our metrics for success."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPxe2qYxIG7EblrvD1C4Pmv",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.17 ('hf-model')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "89e384cab7c47fb35ec95d2248b519cf922ee174880eed636c26cdfb6c4df768"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
